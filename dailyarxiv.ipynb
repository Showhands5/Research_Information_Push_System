{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "#import pymysql\n",
    "from collections import Counter\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "import smtplib\n",
    "from smtplib import SMTP\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.header import Header\n",
    "'''\n",
    "\n",
    "def get_one_page(url):\n",
    "    response = requests.get(url)\n",
    "    print(response.status_code) \n",
    "    while response.status_code == 403:\n",
    "        time.sleep(500 + random.uniform(0, 500))\n",
    "        response = requests.get(url)\n",
    "        print(response.status_code)\n",
    "    print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sys.path[0])\n",
    "url = 'https://arxiv.org/list/cs/pastweek?show=1000'\n",
    "html = get_one_page(url)\n",
    "soup = BeautifulSoup(html, features='html.parser')\n",
    "content = soup.dl\n",
    "date = soup.find('h3')\n",
    "list_ids = content.find_all('a', title = 'Abstract')\n",
    "list_title = content.find_all('div', class_ = 'list-title mathjax')\n",
    "list_authors = content.find_all('div', class_ = 'list-authors')\n",
    "list_subjects = content.find_all('div', class_ = 'list-subjects')\n",
    "list_subjects_split = []\n",
    "list_authors_split = []\n",
    "list_title_split = []\n",
    "# modify title\n",
    "for title in list_title:\n",
    "    title = title.text.split(': ', maxsplit=1)[1]\n",
    "    list_title_split.append(title)\n",
    "\n",
    "# modify subject\n",
    "for subjects in list_subjects:\n",
    "    subjects = subjects.text.split(': ', maxsplit=1)[1]\n",
    "    subjects = subjects.replace('\\n\\n', '')\n",
    "    subjects = subjects.replace('\\n', '')\n",
    "    subject_split = subjects.split('; ')\n",
    "    list_subjects_split.append(subject_split)\n",
    "\n",
    "# modify author\n",
    "for authors in list_authors:\n",
    "    authors = authors.text.split(':', maxsplit=1)[1]\n",
    "    authors = authors.replace('\\n\\n', '')\n",
    "    authors = authors.replace('\\n', '')\n",
    "    authors_splited = authors.split(', ')\n",
    "    list_authors_split.append(authors_splited)\n",
    "    \n",
    "\n",
    "items = []\n",
    "for i, paper in enumerate(zip(list_ids, list_title_split, list_authors_split, list_subject_split)):\n",
    "    items.append([paper[0].text, paper[1], paper[2], paper[3]])\n",
    "name = ['id', 'title', 'author', 'subject']\n",
    "paper = pd.DataFrame(columns=name,data=items)\n",
    "paper.to_csv('./'+time.strftime(\"%Y-%m-%d\")+'_'+str(len(items))+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''subject split'''\n",
    "subject_all = []\n",
    "for subject_split in list_subject_split:\n",
    "    for subject in subject_split:\n",
    "        subject_all.append(subject)\n",
    "subject_cnt = Counter(subject_all)\n",
    "#print(subject_cnt)\n",
    "subject_items = []\n",
    "for subject_name, times in subject_cnt.items():\n",
    "    subject_items.append([subject_name, times])\n",
    "subject_items = sorted(subject_items, key=lambda subject_items: subject_items[1], reverse=True)\n",
    "name = ['name', 'times']\n",
    "subject_file = pd.DataFrame(columns=name,data=subject_items)\n",
    "#subject_file = pd.DataFrame.from_dict(subject_cnt, orient='index')\n",
    "subject_file.to_csv('/home/zzh/Code/Spider/paperspider/arxiv/sub_cnt/'+time.strftime(\"%Y-%m-%d\")+'_'+str(len(items))+'.csv')\n",
    "#subject_file.to_html('subject_file.html')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''key_word1 selection'''\n",
    "key_words = ['track', 'occlu', 'multiple object', 'multiple target', 'multi-object', 'multi-target', 'people', 'person', 'pedestrian', 'human', 'siam'] \n",
    "Key_words = ['MOT', 'SOT']\n",
    "key_words2 = ['quantization', 'compress', 'prun']\n",
    "Key_words2 = ['MOT']    \n",
    "selected_papers = paper[paper['title'].str.contains(key_words[0], case=False)]\n",
    "for key_word in key_words[1:]:\n",
    "    selected_paper1 = paper[paper['title'].str.contains(key_word, case=False)]\n",
    "    selected_papers = pd.concat([selected_papers, selected_paper1], axis=0)\n",
    "for Key_word in Key_words[1:]:\n",
    "    selected_paper1 = paper[paper['title'].str.contains(Key_word, case=True)]\n",
    "    selected_papers = pd.concat([selected_papers, selected_paper1], axis=0)\n",
    "selected_papers.to_csv('./'+time.strftime(\"%Y-%m-%d\")+'_'+str(len(selected_papers))+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email\n",
    "'''\n",
    "## key_word2 selection\n",
    "selected_papers2 = paper[paper['title'].str.contains(key_words2[0], case=False)]\n",
    "for key_word in key_words2[1:]:\n",
    "    selected_paper1 = paper[paper['title'].str.contains(key_word, case=False)]\n",
    "    selected_papers2 = pd.concat([selected_papers2, selected_paper1], axis=0)\n",
    "for Key_word in Key_words2[1:]:\n",
    "    selected_paper1 = paper[paper['title'].str.contains(Key_word, case=True)]\n",
    "    selected_papers2 = pd.concat([selected_papers2, selected_paper1], axis=0)\n",
    "selected_papers2.to_csv('/home/zzh/Code/Spider/paperspider/arxiv/selected2/'+time.strftime(\"%Y-%m-%d\")+'_'+str(len(selected_papers2))+'.csv')\n",
    "'''\n",
    "\n",
    "\n",
    "'''send email'''\n",
    "#selected_papers.to_html('email.html')\n",
    "content = 'Today arxiv has {} new papers in CS area, and {} of them is about CV, {}/{} of them contain your keywords.\\n\\n'.format(len(list_title), subject_cnt['Computer Vision and Pattern Recognition (cs.CV)'], len(selected_papers), len(selected_papers2))\n",
    "content += 'Ensure your keywords is ' + str(key_words) + ' and ' + str(Key_words) + '(case=True). \\n\\n'\n",
    "content += 'This is your paperlist.Enjoy! \\n\\n'\n",
    "for i, selected_paper in enumerate(zip(selected_papers['id'], selected_papers['title'], selected_papers['authors'], selected_papers['subject_split'])):\n",
    "    #print(content1)\n",
    "    content1, content2, content3, content4 = selected_paper\n",
    "    content += '------------' + str(i+1) + '------------\\n' + content1 + content2 + str(content4) + '\\n'\n",
    "    content1 = content1.split(':', maxsplit=1)[1]\n",
    "    content += 'https://arxiv.org/abs/' + content1 + '\\n\\n'\n",
    "\n",
    "content += 'Ensure your keywords2 is ' + str(key_words2) + ' and ' + str(Key_words2) + '(case=True). \\n\\n'\n",
    "content += 'This is your paperlist.Enjoy! \\n\\n'\n",
    "for i, selected_paper2 in enumerate(zip(selected_papers2['id'], selected_papers2['title'], selected_papers2['authors'], selected_papers2['subject_split'])):\n",
    "\n",
    "    #print(content1)\n",
    "    content1, content2, content3, content4 = selected_paper2\n",
    "    content += '------------' + str(i+1) + '------------\\n' + content1 + content2 + str(content4) + '\\n'\n",
    "    content1 = content1.split(':', maxsplit=1)[1]\n",
    "    content += 'https://arxiv.org/abs/' + content1 + '\\n\\n'\n",
    "\n",
    "\n",
    "content += 'Here is the Research Direction Distribution Report. \\n\\n'\n",
    "for subject_name, times in subject_items:\n",
    "    content += subject_name + '   ' + str(times) +'\\n'\n",
    "title = time.strftime(\"%Y-%m-%d\") + ' you have {}+{} papers'.format(len(selected_papers), len(selected_papers2))\n",
    "send_email(title, content)\n",
    "freport = open('/home/zzh/Code/Spider/paperspider/arxiv/report/'+title+'.txt', 'w')\n",
    "freport.write(content)\n",
    "freport.close()\n",
    "\n",
    "'''\n",
    "# dowdload key_word selected papers\n",
    "list_subject_split = []\n",
    "if not os.path.exists('/home/zzh/Code/Spider/paperspider/arxiv/selected/'+time.strftime(\"%Y-%m-%d\")):\n",
    "    os.makedirs('/home/zzh/Code/Spider/paperspider/arxiv/selected/'+time.strftime(\"%Y-%m-%d\"))\n",
    "for selected_paper_id, selected_paper_title in zip(selected_papers['id'], selected_papers['title']):\n",
    "    selected_paper_id = selected_paper_id.split(':', maxsplit=1)[1]\n",
    "    selected_paper_title = selected_paper_title.split(':', maxsplit=1)[1]\n",
    "    r = requests.get('https://arxiv.org/pdf/' + selected_paper_id) \n",
    "    while r.status_code == 403:\n",
    "        time.sleep(500 + random.uniform(0, 500))\n",
    "        r = requests.get('https://arxiv.org/pdf/' + selected_paper_id)\n",
    "    selected_paper_id = selected_paper_id.replace(\".\", \"_\")\n",
    "    pdfname = selected_paper_title.replace(\"/\", \"_\")   #pdf名中不能出现/和：\n",
    "    pdfname = pdfname.replace(\"?\", \"_\")\n",
    "    pdfname = pdfname.replace(\"\\\"\", \"_\")\n",
    "    pdfname = pdfname.replace(\"*\",\"_\")\n",
    "    pdfname = pdfname.replace(\":\",\"_\")\n",
    "    pdfname = pdfname.replace(\"\\n\",\"\")\n",
    "    pdfname = pdfname.replace(\"\\r\",\"\")\n",
    "    print('/home/zzh/Code/Spider/paperspider/arxiv/selected/'+time.strftime(\"%Y-%m-%d\")+'/%s %s.pdf'%(selected_paper_id, selected_paper_title))\n",
    "    with open('/home/zzh/Code/Spider/paperspider/arxiv/selected/'+time.strftime(\"%Y-%m-%d\")+'/%s %s.pdf'%(selected_paper_id,pdfname), \"wb\") as code:    \n",
    "        code.write(r.content)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200\n200\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nlist_ids = content.find_all(\\'a\\', title = \\'Abstract\\')\\nlist_title = content.find_all(\\'div\\', class_ = \\'list-title mathjax\\')\\nlist_authors = content.find_all(\\'div\\', class_ = \\'list-authors\\')\\nlist_subjects = content.find_all(\\'div\\', class_ = \\'list-subjects\\')\\nlist_subjects_split = []\\nlist_authors_split = []\\nlist_title_split = []\\n# modify title\\nfor title in list_title:\\n    title = title.text.split(\\': \\', maxsplit=1)[1]\\n    list_title_split.append(title)\\n\\n# modify subject\\nfor subjects in list_subjects:\\n    subjects = subjects.text.split(\\': \\', maxsplit=1)[1]\\n    subjects = subjects.replace(\\'\\n\\n\\', \\'\\')\\n    subjects = subjects.replace(\\'\\n\\', \\'\\')\\n    subject_split = subjects.split(\\'; \\')\\n    list_subjects_split.append(subject_split)\\n\\n# modify author\\nfor authors in list_authors:\\n    authors = authors.text.split(\\':\\', maxsplit=1)[1]\\n    authors = authors.replace(\\'\\n\\n\\', \\'\\')\\n    authors = authors.replace(\\'\\n\\', \\'\\')\\n    authors_splited = authors.split(\\', \\')\\n    list_authors_split.append(authors_splited)\\n    \\n\\nitems = []\\nfor i, paper in enumerate(zip(list_ids, list_title_split, list_authors_split, list_subject_split)):\\n    items.append([paper[0].text, paper[1], paper[2], paper[3]])\\nname = [\\'id\\', \\'title\\', \\'author\\', \\'subject\\']\\npaper = pd.DataFrame(columns=name,data=items)\\npaper.to_csv(\\'./\\'+time.strftime(\"%Y-%m-%d\")+\\'_\\'+str(len(items))+\\'.csv\\')'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# crasp subject to form table\n",
    "\n",
    "os.chdir(sys.path[0])\n",
    "#url = 'https://arxiv.org/list/cs/pastweek?show=1000'\n",
    "url = 'https://arxiv.org'\n",
    "html = get_one_page(url)\n",
    "soup = BeautifulSoup(html, features='html.parser')\n",
    "content = soup.dl\n",
    "\n",
    "\n",
    "'''\n",
    "list_ids = content.find_all('a', title = 'Abstract')\n",
    "list_title = content.find_all('div', class_ = 'list-title mathjax')\n",
    "list_authors = content.find_all('div', class_ = 'list-authors')\n",
    "list_subjects = content.find_all('div', class_ = 'list-subjects')\n",
    "list_subjects_split = []\n",
    "list_authors_split = []\n",
    "list_title_split = []\n",
    "# modify title\n",
    "for title in list_title:\n",
    "    title = title.text.split(': ', maxsplit=1)[1]\n",
    "    list_title_split.append(title)\n",
    "\n",
    "# modify subject\n",
    "for subjects in list_subjects:\n",
    "    subjects = subjects.text.split(': ', maxsplit=1)[1]\n",
    "    subjects = subjects.replace('\\n\\n', '')\n",
    "    subjects = subjects.replace('\\n', '')\n",
    "    subject_split = subjects.split('; ')\n",
    "    list_subjects_split.append(subject_split)\n",
    "\n",
    "# modify author\n",
    "for authors in list_authors:\n",
    "    authors = authors.text.split(':', maxsplit=1)[1]\n",
    "    authors = authors.replace('\\n\\n', '')\n",
    "    authors = authors.replace('\\n', '')\n",
    "    authors_splited = authors.split(', ')\n",
    "    list_authors_split.append(authors_splited)\n",
    "    \n",
    "\n",
    "items = []\n",
    "for i, paper in enumerate(zip(list_ids, list_title_split, list_authors_split, list_subject_split)):\n",
    "    items.append([paper[0].text, paper[1], paper[2], paper[3]])\n",
    "name = ['id', 'title', 'author', 'subject']\n",
    "paper = pd.DataFrame(columns=name,data=items)\n",
    "paper.to_csv('./'+time.strftime(\"%Y-%m-%d\")+'_'+str(len(items))+'.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'dl'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8827804e3a4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpsubjects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpsubjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1618\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1619\u001b[0m         raise AttributeError(\n\u001b[1;32m-> 1620\u001b[1;33m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1621\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'dl'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "psubjects = soup.find_all('h2')\n",
    "print(psubjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Artificial Intelligence\nComputation and Language\nComputational Complexity\nComputational Engineering, Finance, and Science\nComputational Geometry\nComputer Science and Game Theory\nComputer Vision and Pattern Recognition\nComputers and Society\nCryptography and Security\nData Structures and Algorithms\nDatabases\nDigital Libraries\nDiscrete Mathematics\nDistributed, Parallel, and Cluster Computing\nEmerging Technologies\nFormal Languages and Automata Theory\nGeneral Literature\nGraphics\nHardware Architecture\nHuman-Computer Interaction\nInformation Retrieval\nInformation Theory\nLogic in Computer Science\nMachine Learning\nMathematical Software\nMultiagent Systems\nMultimedia\nNetworking and Internet Architecture\nNeural and Evolutionary Computing\nNumerical Analysis\nOperating Systems\nOther Computer Science\nPerformance\nProgramming Languages\nRobotics\nSocial and Information Networks\nSoftware Engineering\nSound\nSymbolic Computation\nSystems and Control\n"
    }
   ],
   "source": [
    "#def getSubjects(pSubjectNameAbbr, )\n",
    "pSubjectAbbr = 'cs'\n",
    "subjects = soup.find_all('a', id=re.compile(r'cs.[A-Z]{2}') ) #, class_=re.compile(r'cs.[A-Z]{2}')\n",
    "#print(subjects)\n",
    "for s in subjects:\n",
    "    print(s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form .csv\n",
    "def saveCSV(subjects, psubjectId, psubjectName):\n",
    "    items = []\n",
    "    for i, s in enumerate(subjects):\n",
    "        items.append([s.text, psubjectId])\n",
    "    name = ['subject', 'psubject']\n",
    "    paper = pd.DataFrame(columns=name,data=items,index=None)\n",
    "    paper.to_csv('./'+psubjectName+'_'+str(len(items))+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCSV(subjects, 1, 'Computer Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id                                             name  pid\n0   1                                 Computer Science    0\n1   6                          Artificial Intelligence    1\n2   7                         Computation and Language    1\n3   8                         Computational Complexity    1\n4   9  Computational Engineering, Finance, and Science    1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>pid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Computer Science</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>Artificial Intelligence</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>Computation and Language</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>Computational Complexity</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>Computational Engineering, Finance, and Science</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "pdSubjects = pd.read_csv('./subject.csv')\n",
    "pdSubjects.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200\n200\n"
    }
   ],
   "source": [
    "os.chdir(sys.path[0])\n",
    "url = 'https://arxiv.org/list/cs/new'\n",
    "html = get_one_page(url)\n",
    "soup = BeautifulSoup(html, features='html.parser')\n",
    "content = soup.dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     id                       url  \\\n0  2380  arxiv.org/abs/2005.02380   \n1  2384  arxiv.org/abs/2005.02384   \n2  2387  arxiv.org/abs/2005.02387   \n3  2389  arxiv.org/abs/2005.02389   \n4  2390  arxiv.org/abs/2005.02390   \n\n                                               title  \\\n0  Bit-Interleaved Coded Multiple Beamforming wit...   \n1                Compositionality of the MSO+U Logic   \n2  SurvLIME-Inf: A simplified modification of Sur...   \n3  Jointly Sparse Support Recovery via Deep Auto-...   \n4                   Mechanism Design and Blockchains   \n\n                                            author subject  \n0                    Sadjad Sedighi,Ender Ayanoglu    [27]  \n1                                      Paweł Parys    [28]  \n2  Lev V. Utkin,Maxim S. Kovalev,Ernest M. Kasimov    [29]  \n3              Wanqing Zhang,Shuaichao Li,Ying Cui    [27]  \n4       Akaki Mamageishvili,Jan Christoph Schlegel    [11]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url</th>\n      <th>title</th>\n      <th>author</th>\n      <th>subject</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2380</td>\n      <td>arxiv.org/abs/2005.02380</td>\n      <td>Bit-Interleaved Coded Multiple Beamforming wit...</td>\n      <td>Sadjad Sedighi,Ender Ayanoglu</td>\n      <td>[27]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2384</td>\n      <td>arxiv.org/abs/2005.02384</td>\n      <td>Compositionality of the MSO+U Logic</td>\n      <td>Paweł Parys</td>\n      <td>[28]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2387</td>\n      <td>arxiv.org/abs/2005.02387</td>\n      <td>SurvLIME-Inf: A simplified modification of Sur...</td>\n      <td>Lev V. Utkin,Maxim S. Kovalev,Ernest M. Kasimov</td>\n      <td>[29]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2389</td>\n      <td>arxiv.org/abs/2005.02389</td>\n      <td>Jointly Sparse Support Recovery via Deep Auto-...</td>\n      <td>Wanqing Zhang,Shuaichao Li,Ying Cui</td>\n      <td>[27]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2390</td>\n      <td>arxiv.org/abs/2005.02390</td>\n      <td>Mechanism Design and Blockchains</td>\n      <td>Akaki Mamageishvili,Jan Christoph Schlegel</td>\n      <td>[11]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# save paper\n",
    "\n",
    "date = soup.find('h3')\n",
    "list_ids = content.find_all('a', title = 'Abstract')\n",
    "list_title = content.find_all('div', class_ = 'list-title mathjax')\n",
    "list_authors = content.find_all('div', class_ = 'list-authors')\n",
    "list_subjects = content.find_all('div', class_ = 'list-subjects')\n",
    "list_subjects_split = []\n",
    "list_authors_split = []\n",
    "list_title_split = []\n",
    "list_id = []\n",
    "list_url_split = []\n",
    "# modify url\n",
    "for i in list_ids:\n",
    "    \n",
    "    i = i.text.split(':', maxsplit=1)[1]\n",
    "    i.strip()\n",
    "    list_id.append(int(i.split('.')[1]))\n",
    "    list_url_split.append('arxiv.org/abs/'+i)\n",
    "\n",
    "# modify title\n",
    "for title in list_title:\n",
    "    title = title.text.split(': ', maxsplit=1)[1]\n",
    "    title = title.replace('\\n', '')\n",
    "    list_title_split.append(title)\n",
    "\n",
    "# modify subject\n",
    "for subjects in list_subjects:\n",
    "    subjects = subjects.text.split(': ', maxsplit=1)[1]\n",
    "    subjects = subjects.replace('\\n\\n', '')\n",
    "    subjects = subjects.replace('\\n', '')\n",
    "    subject_split = subjects.split('; ')\n",
    "    subject_split = [i.split(' (')[0] for i in subject_split]\n",
    "    #print(subject_split)\n",
    "    ids = pdSubjects[pdSubjects['name'].isin(subject_split)]['id']\n",
    "    #print(ids)\n",
    "    ids = list(ids)\n",
    "    list_subjects_split.append(ids)\n",
    "\n",
    "# modify author\n",
    "for authors in list_authors:\n",
    "    authors = authors.text.split(':', maxsplit=1)[1]\n",
    "    authors = authors.replace('\\n\\n', '')\n",
    "    authors = authors.replace('\\n', '')\n",
    "    #authors_splited = authors.split(', ')\n",
    "    #authors.replace(re.compile(r'\\(\\w+\\)'), '')\n",
    "    #list_authors_split.append(authors_splited)\n",
    "    authors = authors.replace(', ', ',')\n",
    "    list_authors_split.append(authors)\n",
    "    \n",
    "\n",
    "items = []\n",
    "for i, paper in enumerate(zip(list_id, list_url_split, list_title_split, list_authors_split, list_subjects_split)):\n",
    "    items.append([paper[0], paper[1], paper[2], paper[3], paper[4]])\n",
    "name = ['id', 'url', 'title', 'author', 'subject']\n",
    "paper = pd.DataFrame(columns=name,data=items)\n",
    "paperCSVName = \n",
    "paper.to_csv('./paper_'+time.strftime(\"%Y-%m-%d\")+'_'+str(len(items))+'.csv')\n",
    "paper.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_subject_items = []\n",
    "for item in items:\n",
    "    for subject in item[4]:\n",
    "        paper_subject_items.append([item[0], subject])\n",
    "paper_subject = pd.DataFrame(columns=['paper_id', 'subject_id'], data=paper_subject_items)\n",
    "paper_subject.to_csv('./paper_subject_'+time.strftime(\"%Y-%m-%d\")+'_'+str(len(items))+'.csv', index=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     id                                              title\n0  1892  Bit-Interleaved Coded Multiple Beamforming wit...\n1  1893                Compositionality of the MSO+U Logic\n2  1894  SurvLIME-Inf: A simplified modification of Sur...\n3  1895  Jointly Sparse Support Recovery via Deep Auto-...\n4  1896                   Mechanism Design and Blockchains",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1892</td>\n      <td>Bit-Interleaved Coded Multiple Beamforming wit...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1893</td>\n      <td>Compositionality of the MSO+U Logic</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1894</td>\n      <td>SurvLIME-Inf: A simplified modification of Sur...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1895</td>\n      <td>Jointly Sparse Support Recovery via Deep Auto-...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1896</td>\n      <td>Mechanism Design and Blockchains</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "dfPaper = pd.read_csv('./paper.csv')\n",
    "dfPaper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = ['paper_id', 'subject_id']\n",
    "items = []\n",
    "for i in dfPaper:\n",
    "    for j in i['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Unnamed: 0                       url  \\\n1           1  arxiv.org/abs/2005.02384   \n2           2  arxiv.org/abs/2005.02387   \n3           3  arxiv.org/abs/2005.02389   \n4           4  arxiv.org/abs/2005.02390   \n5           5  arxiv.org/abs/2005.02392   \n\n                                               title  \\\n1                Compositionality of the MSO+U Logic   \n2  SurvLIME-Inf: A simplified modification of Sur...   \n3  Jointly Sparse Support Recovery via Deep Auto-...   \n4                   Mechanism Design and Blockchains   \n5  Deep Lagrangian Constraint-based Propagation i...   \n\n                                              author subject  \n1                                        Paweł Parys    [28]  \n2    Lev V. Utkin,Maxim S. Kovalev,Ernest M. Kasimov    [29]  \n3                Wanqing Zhang,Shuaichao Li,Ying Cui    [27]  \n4         Akaki Mamageishvili,Jan Christoph Schlegel    [11]  \n5  Matteo Tiezzi,Giuseppe Marra,Stefano Melacci,M...    [29]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>url</th>\n      <th>title</th>\n      <th>author</th>\n      <th>subject</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>arxiv.org/abs/2005.02384</td>\n      <td>Compositionality of the MSO+U Logic</td>\n      <td>Paweł Parys</td>\n      <td>[28]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>arxiv.org/abs/2005.02387</td>\n      <td>SurvLIME-Inf: A simplified modification of Sur...</td>\n      <td>Lev V. Utkin,Maxim S. Kovalev,Ernest M. Kasimov</td>\n      <td>[29]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>arxiv.org/abs/2005.02389</td>\n      <td>Jointly Sparse Support Recovery via Deep Auto-...</td>\n      <td>Wanqing Zhang,Shuaichao Li,Ying Cui</td>\n      <td>[27]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>arxiv.org/abs/2005.02390</td>\n      <td>Mechanism Design and Blockchains</td>\n      <td>Akaki Mamageishvili,Jan Christoph Schlegel</td>\n      <td>[11]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>arxiv.org/abs/2005.02392</td>\n      <td>Deep Lagrangian Constraint-based Propagation i...</td>\n      <td>Matteo Tiezzi,Giuseppe Marra,Stefano Melacci,M...</td>\n      <td>[29]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200\n200\n"
    }
   ],
   "source": [
    "# activity\n",
    "os.chdir(sys.path[0])\n",
    "url = 'http://www.allconferences.com/search/index/Category__parent_id:12290/Conference__start_date__from:05-01-2020/Conference__start_date__to:11-01-2020/showLastConference:1/'\n",
    "html = get_one_page(url)\n",
    "soup = BeautifulSoup(html, features='html.parser')\n",
    "content = soup.dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n===========\n"
    }
   ],
   "source": [
    "#print(soup)\n",
    "conferences = soup.find_all('div', id=re.compile(r'conference_\\d+'))\n",
    "'''\n",
    "begins = soup.find_all('span', class_='begin_txt')\n",
    "ends = soup.find_all('span', class_='end_txt')\n",
    "namesAndUrls = soup.find_all('a', href = re.compile(r'http://www\\.allconferences\\.com[\\s\\S]*'))\n",
    "'''#date = soup.find('h3')\n",
    "#print(namesAndUrls)\n",
    "def timeTransfer(origin):\n",
    "    tmp = time.strptime(origin, \"%b %d, %Y\")\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", tmp)\n",
    "activities = []\n",
    "for c in conferences:\n",
    "    beginTime = c.find('span', class_='begin_txt').text.split('\\n',maxsplit=2)[1]\n",
    "    beginTime = timeTransfer(beginTime)\n",
    "    endTime = c.find('span', class_='end_txt').text.split('\\r\\n',maxsplit=1)[1].strip()\n",
    "    #print(beginTime)\n",
    "    endTime = timeTransfer(endTime)\n",
    "    url = c.find('h2').find('a')\n",
    "    #print(url)\n",
    "    name = url.text\n",
    "    if name == None:\n",
    "        continue\n",
    "    #print(name)\n",
    "    url = url.get('href')\n",
    "    #name = url.text\n",
    "    #print(beginTime, endTime, url, name)\n",
    "    print(\"===========\")\n",
    "    activities.append([name, url, beginTime, endTime])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                name  \\\n0  IEEE--2020 The 5th International Conference on...   \n1  ACM--2020 the 4th Int. Conference on Innovatio...   \n2  2020 The 7th International Conference on Digit...   \n3  2020 International Conference on Wireless Comm...   \n4  2020 World Conference on Computing and Communi...   \n\n                                                 url            beginTime  \\\n0  http://www.allconferences.com/c/ieee-2020-the-...  2020-05-08 00:00:00   \n1  http://www.allconferences.com/c/acm-2020-the-4...  2020-05-08 00:00:00   \n2  http://www.allconferences.com/c/2020-the-7th-i...  2020-05-10 00:00:00   \n3  http://www.allconferences.com/c/2020-internati...  2020-05-13 00:00:00   \n4  http://www.allconferences.com/c/2020-world-con...  2020-05-13 00:00:00   \n\n               endTime  \n0  2020-05-11 00:00:00  \n1  2020-05-11 00:00:00  \n2  2020-05-12 00:00:00  \n3  2020-05-15 00:00:00  \n4  2020-05-15 00:00:00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>url</th>\n      <th>beginTime</th>\n      <th>endTime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>IEEE--2020 The 5th International Conference on...</td>\n      <td>http://www.allconferences.com/c/ieee-2020-the-...</td>\n      <td>2020-05-08 00:00:00</td>\n      <td>2020-05-11 00:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ACM--2020 the 4th Int. Conference on Innovatio...</td>\n      <td>http://www.allconferences.com/c/acm-2020-the-4...</td>\n      <td>2020-05-08 00:00:00</td>\n      <td>2020-05-11 00:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020 The 7th International Conference on Digit...</td>\n      <td>http://www.allconferences.com/c/2020-the-7th-i...</td>\n      <td>2020-05-10 00:00:00</td>\n      <td>2020-05-12 00:00:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020 International Conference on Wireless Comm...</td>\n      <td>http://www.allconferences.com/c/2020-internati...</td>\n      <td>2020-05-13 00:00:00</td>\n      <td>2020-05-15 00:00:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020 World Conference on Computing and Communi...</td>\n      <td>http://www.allconferences.com/c/2020-world-con...</td>\n      <td>2020-05-13 00:00:00</td>\n      <td>2020-05-15 00:00:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "\n",
    "activity = pd.DataFrame(columns=['name', 'url', 'beginTime', 'endTime'],data=activities)\n",
    "activity.to_csv('./activity_'+time.strftime(\"%Y-%m-%d\")+'_'+str(len(activities))+'.csv',index=None)\n",
    "activity.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}